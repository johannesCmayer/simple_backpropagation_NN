# -*- coding: utf-8 -*-
"""Backprop.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E_T4EUUgK7s8b_5cfRQV8X9hv55y7yAg
"""
import copy
import numpy as np
import matplotlib.pyplot as plt
import typing as tp
import random
from matplotlib.animation import FuncAnimation


class InputException(Exception):
    def __init__(self, expectet, actual):
        super().__init__()('expected input {} but got {}'.format(expected, actual))

def mtp():
    print('asnoteuhasnotehuasnotehu')
    raise Exception('this should be patched')

def norm(*args, factor=2):
    return np.power(sum([a ** 2 for a in args]), 1 / factor)


def relu(x):
    return np.maximum(0, x)


def l_relu(x, seal_strength=10):
    return x * ((x <= 0).astype(np.float32) / seal_strength + (x > 0).astype(np.float32))


def mean_square_error(predictions, labels):
    return np.sum([np.power(p - l, 2) for p, l in zip(predictions, labels)])


class Network:
    def __init__(self, num_neurons, dx=0.01):
        self._validate_input(num_neurons, 1)
        if len(num_neurons) < 2: raise Exception('network needs at least 2 layers.', num_neurons)
        if not any(num_neurons): raise Exception('network layer needs at least 1 neuron.', num_neurons)
        creation_func = lambda *args: np.random.rand(*args) * 1
        self.weights = [creation_func(e, num_neurons[i]) for i, e in enumerate(num_neurons[1:])]
        self.biasses = np.array([creation_func(a) for a in num_neurons[1:]])
        self.dx = dx
        self._validate()

    @property
    def weights(self):
        return self._weights

    @weights.setter
    def weights(self, v):
        self._weights = v

    @property
    def biasses(self):
        return self._biasses

    @biasses.setter
    def biasses(self, v):
        self._biasses = v

    def eval(self, input: np.array, or_weights=None, or_biasses=None, nonlin=relu):
        self._validate_input(input, 1)
        if or_weights is None:
            or_weights = self.weights
        if or_biasses is None:
            or_biasses = self.biasses
        for weight, bias in zip(or_weights, or_biasses):
            input = nonlin(weight @ input + bias)
        return np.array(input)

    def batch_eval(self, inputs, **kwargs):
        self._validate_input(inputs, 2)
        outputs = []
        for sample in inputs:
            outputs.append(self.eval(sample, **kwargs))
        return np.array(outputs)

    def _weight_gradient(self, input, label):
        self._validate_input(input, 1)
        network_gradient = []
        for i_l, layer in enumerate(self.weights):
            layer_gradients = []
            for i_n, neuron in enumerate(layer):
                neuron_gradients = []
                for i_w, weight in enumerate(neuron):
                    custom_weights = copy.deepcopy(self.weights)
                    custom_weights[i_l][i_n][i_w] = custom_weights[i_l][i_n][i_w] + self.dx
                    output = mean_square_error(self.eval(input), label)
                    nudged_output = mean_square_error(self.eval(input, or_weights=custom_weights), label)
                    weight_grad = (output - nudged_output) / self.dx
                    neuron_gradients.append(weight_grad)
                layer_gradients.append(neuron_gradients)
            network_gradient.append(layer_gradients)
        return network_gradient

    def _bias_gradient(self, input, label):
        self._validate_input(input, 1)
        layer_gradient = []
        for i_l, layer in enumerate(self.biasses):
            neuron_gradient = []
            for i_n, neuron in enumerate(layer):
                custom_biasses = copy.deepcopy(self.biasses)
                custom_biasses[i_l][i_n] = custom_biasses[i_l][i_n] + self.dx
                output = mean_square_error(self.eval(input), label)
                nudged_output = mean_square_error(self.eval(input, or_biasses=custom_biasses), label)
                bias_grad = (output - nudged_output) / self.dx
                neuron_gradient.append(bias_grad)
            layer_gradient.append(neuron_gradient)
        return layer_gradient

    def average_weights(self, total_weight_gradient):
        layer_average = []
        for i, layer in enumerate(total_weight_gradient[0]):
            neuron_average = []
            for j, neuron in enumerate(layer):
                weight_average = []
                for k, weight in enumerate(neuron):
                    weight_batch_average = []
                    for l, _ in enumerate(total_weight_gradient):
                        weight_batch_average.append(total_weight_gradient[l][i][j][k])
                    weight_average.append(sum(weight_batch_average) / len(total_weight_gradient))
                neuron_average.append(weight_average)
            layer_average.append(np.array(neuron_average))
        return layer_average

    def average_biasses(self, total_bias_gradient):
        layer_average = []
        for i, layer in enumerate(total_bias_gradient[0]):
            neuron_average = []
            for j, neuron in enumerate(layer):
                bias_batch_average = []
                for l, _ in enumerate(total_bias_gradient):
                    bias_batch_average.append(total_bias_gradient[l][i][j])
                neuron_average.append(sum(bias_batch_average) / len(total_bias_gradient))
            layer_average.append(np.array(neuron_average))
        return layer_average

    def total_gradient(self, inputs, labels, batch_size=None):
        self._validate_input(inputs, 2)
        data = zip(inputs, labels)
        if batch_size is not None:
            data = random.sample(data, batch_size)
        total_weight_gradient = []
        total_bias_gradient = []
        for input, label in data:
            total_weight_gradient.append(self._weight_gradient(input, label))
            total_bias_gradient.append(self._bias_gradient(input, label))
        return self.average_weights(total_weight_gradient), self.average_biasses(total_bias_gradient)

    def optimize(self, inputs, labels, iterations, learning_rate=5e-4, play_learn_animation=True):
        self._validate_input(inputs, 2)
        print_frequency = iterations / 10
        for i in range(iterations):
            weight_grad, bias_grad = self.total_gradient(inputs, labels)
            self.weights = [layer_w + w_update * learning_rate for layer_w, w_update in
                            zip(self.weights, weight_grad)]
            self.biasses = [layer_b + b_update * learning_rate for layer_b, b_update in
                            zip(self.biasses, bias_grad)]
            if (i % print_frequency == 0 or i == 0):
                print('iter:', i)
                print('error:', mean_square_error(self.batch_eval(inputs), labels))
        self._validate()
        if play_learn_animation:
            learn_animation((inputs, self.batch_eval(inputs)), (inputs, labels))

    def _validate(self):
        pass
        #if depth(self.weights) != 3:
        #    raise Exception('weight shape needs 3 dim, has', depth(self.weights), "Value is:", self.weights)
        #if np.ndim(self.biasses) != 2:
        #    raise Exception('biasses shape needs 2 dim, but has {}'.format(np.shape(self.biasses)))

    def _validate_input(self, input, correct_input_dim):
        if np.ndim(input) != correct_input_dim:
            raise Exception('Input needs to have dim {}'.format(correct_input_dim))


def run():
    labels = np.sin(np.linspace(5, 10, 20).reshape(-1, 1)) * 10 + 20
    #labels = np.ones(20).reshape(-1,1)
    inputs = np.linspace(5, 10, 20).reshape(-1, 1)

    n = Network((len(inputs[0]), 5, 5, len(labels[0])))
    n.optimize(inputs, labels, 100, play_learn_animation=True)

    results = []
    for i in inputs:
        results.append(n.eval(i))
    print(results)
    plt.plot(np.reshape(inputs, -1), np.reshape(results, -1), 'b-')
    plt.plot(np.reshape(inputs, -1), np.reshape(labels, -1), 'r-')
    #plt.ylim(0,3)
    #plt.xlim(5,10)
    plt.show()

def learn_animation(*args):
    fig, ax = plt.subplots()
    lines = []
    for cord in args:
        lines.append(plt.plot([], [], 'ro', animated=True))
    lines = [l[0] for l in lines]

    def update(frame):
        frame = int(frame)
        xdata, ydata = args[frame]
        xdata, ydata = np.reshape(xdata, -1), np.reshape(ydata, -1)
        lines[frame].set_data(xdata, ydata)
        return lines

    ani = FuncAnimation(fig, update, frames=np.linspace(0, len(args)-1, len(args)-1), blit=True, repeat=True)
    plt.ylim(0,40)
    plt.xlim(0,40)
    plt.show()

if __name__ == '__main__':
    run()
